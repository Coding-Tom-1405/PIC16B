[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/bruin/HW 5/index.html",
    "href": "posts/bruin/HW 5/index.html",
    "title": "Homework 5: TensorFlow and Keras Modeling (Image Classification)",
    "section": "",
    "text": "In this blog post, we will go over how keras layering and tensorflow to create different models for a dataset. For our example, we will be using image classification to determine the specific type of animal the image portrays: cat or dog.\n\nLoading Packages\nTo start, we need to import and load all our needed packages.\nFirst, because we will be utilizing Keras 3 in order to work on top of our TensorFlow backend, we need to upgrade the Colab’s default version of 2.15.0 to get version 3.0.5\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 16.5 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\nWith this, we can import all the packages and establish our backend:\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport keras\nfrom keras import utils, datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nimport tensorflow_datasets as tfds\nfrom tensorflow import data as tf_data\nimport tensorflow as tf\nimport random\n\nNow that we imported the packages, let’s check the version of keras\n\nkeras.__version__\n\n'3.0.5'\n\n\nNext, since we are creating Keras backends and TensorFlow models, we will incorporate GPUs (or graphics processing units) to accelerate the computation.\nYou’ll need to enable GPUs for the notebook:\n\nNavigate to Edit→Notebook Settings\nselect GPU from the Hardware Accelerator drop-down\n\nAfter, we’ll confirm that we can connect the GPU with jax:\n\nimport jax\njax.devices()\n\n[cuda(id=0)]\n\n\nLet’s check our GPU usage\n\n!nvidia-smi\n\nTue Mar  5 01:01:57 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   62C    P0              26W /  70W |    105MiB / 15360MiB |      2%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\n\n\nObtaining Data\nNow that we have finished loading our packages, we can start creating our datasets using tensorflow.\nSimilar to when using numpy in the past, we will split our data into groups, except this time we will have three of them: train, test, and validation. The reason we have a validation dataset is because we are training multiple models that have different combinations of hyperameters. Thus, we need a common factor that will help evaluate and compare the performance of each model through the validation set.\nFor our code, we will split it we will do 40:10:10 (remaining unused), so our code should look something like this:\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nNext, to ensure consistent sizing, we want to resize and establish the expected dimension of all our images. We can accomplish this by using the layers object and Resizing() function in keras as follows:\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nLastly, we want to set up our data piplines for training, validation, and testing (in order to optimize efficiency of loading and preprocessing our data), which can be done as seen below:\n\n# Defining number of samples processed in each batch for training\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nOnce our batches of data are complete, let’s see what our images look like (checking to see if they are consistent, yet still clear). We can do this by creating a function that will take in our dataset and output the desired-dimensions subplot)\n\ndef ds_visual(dataset, num_rows, num_cols, n):\n\n    # Creating plot\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 7))\n\n    # Selecting a single batch\n    for i, (images, labels) in enumerate(dataset.skip(n).take(1)):\n        # Initializing counters for dog,cat appearances in plot\n        cat_count = 0\n        dog_count = 0\n\n    # Iterating through the batch of image,label pairings\n    for image, label in zip(images, labels):\n        # Selecting only the car images\n        if label == 0 and cat_count &lt; num_cols:\n            # Plotting cat image\n            axes[0, cat_count].imshow(image.numpy().astype(\"uint8\"))\n            axes[0, cat_count].set_title('Cat')\n            axes[0, cat_count].axis(\"off\")\n            cat_count += 1\n\n        # Selecting only the car images\n        elif label == 1 and dog_count &lt; num_cols:\n            # Plotting dog image\n            axes[1, dog_count].imshow(image.numpy().astype(\"uint8\"))\n            axes[1, dog_count].set_title('Dog')\n            axes[1, dog_count].axis(\"off\")\n            dog_count += 1\n\n        # Stopping looop once all subplots filled\n        if cat_count == num_cols and dog_count == num_cols:\n            break\n\n    plt.show()\n\nWith this function, we can see what our images look like.\n\nds_visual(train_ds, 2, 3, random.randint(1,10))\n\n\n\n\n\n\n\n\nPerfect! We have exactly one row of three cats and one row of three dogs, and all of these images are of the same dimension.\nBefore moving on to our training models, let’s first check that our dataset is fair. What we mean by that is because we split the dataset without knowing the distribution of the two species, we need to see if their frequencies are relatively even. Thus, we can check the distribution with the following two codes:\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncat_count = 0\ndog_count = 0\n\nfor label in labels_iterator:\n    if label == 0:  # Cat label\n        cat_count += 1\n    elif label == 1:  # Dog label\n        dog_count += 1\n\nprint(\"Number of images with label 0 (cat):\", cat_count)\nprint(\"Number of images with label 1 (dog):\", dog_count)\n\nNumber of images with label 0 (cat): 4637\nNumber of images with label 1 (dog): 4668\n\n\nGreat! It is even.\nTo have a starting comparing condition, we should first talk about the most basic predicting model: the baseline model.\nBecause the baseline model always predicts the most occurring label (in our case, dog), then the baseline model would only have an accuracy rate of 4668/(4637+4668)%, or 50.2%. Again, this makes sense in that because our dataset is basically proportional, then the baseline ultimately has a 1 in 2 chance of predicting the right label, dog or cat.\nWith that understanding and starting place, let’s jump into creating the models!\n\n\nFirst Model (Basic Model Structure)\nFor our first one, we will start of by introducing the most basic component when building a model.\nTo start off, we need to specify the expected shape of our data, which will require us to put layers.Input() at the beginning of our model.\nNext, with all of our models, we want to extract “features” (meaningful properties) from each images. We can do this by using Conv2D() which adds 2D convolutional layer in our neural network models to magnify the any specific textures or patterns to help with classification.\nWith a component helping magnify features, we also want a component that will help remove irrelavant data. This is done through MaxPooling2D which retains the max-values pixels in different regions while reducing the spatial dimensions of the image (Typically models will have this and Conv2D as alternating layers).\nAfter finishing our “2D” components, we now need to Flatten our data to 1D so that we can pass it through a Dense layer which will make the prediction for us. In addition, to prevent overfitting, we will also want to incorporate a Dropout layer (turning certain amount of input units to 0) at times\nWith all this info, our model should look something like this:\n\nmodel1 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10), # number of classes\n    layers.Dropout(0.5)\n])\n\nThe main thing we experimented with is 1) dropout value and 2) and maxpooling value as both of these have to do with reducing spatial dimension which impacts how much important feature remains.\nTo further understand what our model is doing, let’s take a look at what is happening to our image data at each step.\n\nmodel1.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                      │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (MaxPooling2D)         │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (Conv2D)                    │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (Conv2D)                    │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (Flatten)                    │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nAnd with that, let’s start configuring the training process for our neural network model. To do this, we need to specify what optimization algorithm, loss function (how well model’s predictions match the true labels), and evaluation metric during training. Thus, our code will look something like this:\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nLastly, all that is left to do is to train it. To ensure consistency and/or improvement of our model, we will use the epoch parameter to indicate how many times our model will go through the entire training dataset. Thus, our code will be as follows:\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 20s 90ms/step - accuracy: 0.4883 - loss: 23.6040 - val_accuracy: 0.5537 - val_loss: 0.9793\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 33ms/step - accuracy: 0.5134 - loss: 1.4983 - val_accuracy: 0.5666 - val_loss: 0.7251\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 38ms/step - accuracy: 0.5507 - loss: 1.3817 - val_accuracy: 0.5997 - val_loss: 0.7705\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5766 - loss: 1.3752 - val_accuracy: 0.6028 - val_loss: 0.7648\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5961 - loss: 1.3353 - val_accuracy: 0.5993 - val_loss: 0.7467\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.6193 - loss: 1.2841 - val_accuracy: 0.5903 - val_loss: 0.9021\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.6491 - loss: 1.2248 - val_accuracy: 0.6165 - val_loss: 0.8251\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.6811 - loss: 1.1777 - val_accuracy: 0.6092 - val_loss: 0.8820\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7094 - loss: 1.1061 - val_accuracy: 0.6002 - val_loss: 1.0495\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7332 - loss: 1.0746 - val_accuracy: 0.6079 - val_loss: 1.0947\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7657 - loss: 1.0373 - val_accuracy: 0.6071 - val_loss: 1.0857\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7786 - loss: 1.0003 - val_accuracy: 0.5959 - val_loss: 1.2514\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.7966 - loss: 0.9854 - val_accuracy: 0.6066 - val_loss: 1.3315\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.8142 - loss: 0.9666 - val_accuracy: 0.6109 - val_loss: 1.4496\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8063 - loss: 0.9892 - val_accuracy: 0.6032 - val_loss: 1.4400\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8015 - loss: 1.0174 - val_accuracy: 0.6028 - val_loss: 1.7445\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8122 - loss: 0.9848 - val_accuracy: 0.5980 - val_loss: 1.5714\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8057 - loss: 0.9930 - val_accuracy: 0.5954 - val_loss: 1.8500\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8095 - loss: 0.9867 - val_accuracy: 0.5989 - val_loss: 2.4330\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8218 - loss: 0.9689 - val_accuracy: 0.6088 - val_loss: 2.3539\n\n\nThere are two things that we can note from looking at this statistics. First, we can see that the accuracy of my model stabilized between 55.37% and 61.09% during training, meaning our model is 5.17-10.89% better at predicting the label. However, you will notice that there are two different accuracy values in each epoch. One of them is for the validation dataset while the other one accuracy is for the training dataset. Let’s graph to compare what these accuracies look like agains each other\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nFrom this graph, we can see that the the gap between the two variables are getting increasingly larger, with training accuracy values is being greater than the validation accuracy as we go through more iterations. This indicates an overfitting in our model as it is relying heavily on the features in the training dataset, resulting an underperformance in our validation dataset.\n\n\nModel with Data Augment\nWith the basic model structure out of the way, we can now add more specific components to enhance our model’s performance, starting with data augmentation.\nData augmentation refers to the practice of including modified copies of the same image in the training set, meaning that a dog image will always remain to be a dog image even if we decide to flip or rotate it.By adding the transformed “version” of the images, our model can learn invariant features of our data.\nTo achieve this step, all we need to add are the following to layers in our model: layers.RandomFlip() and layers.RandomRotation(). To see these layers in action, we will plot our original image along with a few copies where the layers were applied.\nFirst, the RandomFlip():\n\ndata_augmentation_random_flip = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\")\n])\n\nfor images, labels in train_ds.take(1):\n    # Taking the first image from the batch\n    image = images[0]\n    label = labels[0]\n\n# Adding the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\nplt.figure(figsize=(10, 10))\n\n# Plotting the original, unflipped image\nplt.subplot(1, 3, 1)\n# Accessing the first element of the batch dimension\nplt.imshow(image[0].numpy().astype(\"uint8\"))\nplt.axis(\"off\")\n\n# Plotting the flipped versions\nfor i in range(2):\n    augmented_image = data_augmentation_random_flip(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, through the RandomFlip layer, we were able to flip our dog image both horizontally and/or vertically.\nNext, the RandomRotation():\n\ndata_augmentation_random_rotate = tf.keras.Sequential([\n  layers.RandomRotation(0.2) # 0.2 specifies max rotation angle\n])\n\nfor images, labels in train_ds.take(1):\n    # Assuming you want the first image from the batch\n    image = images[0]\n    label = labels[0]\n\n# # Add the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\nplt.figure(figsize=(10, 10))\n\n# Plot the original, unflipped image\nplt.subplot(1, 3, 1)\nplt.imshow(image[0].numpy().astype(\"uint8\"))  # Accessing the first element of the batch dimension\nplt.axis(\"off\")\n\n# Plot the flipped versions\nfor i in range(2):\n    augmented_image = data_augmentation_random_rotate(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\nFor this, we can see how our dog has been rotated at various angles.\nWith these two new layers, we can add on to our starting model. Since we want our model to first be able to identify whether or not the image has been modulated, we want to make these two augmentation layers the first ones in our models, making our code look like this:\n\nmodel2 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\"),  # Flipping augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10),\n    layers.Dropout(0.5)\n])\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n\n\nAgain, let’s take a look at our summary and start training.\n\nmodel2.summary()\n\nModel: \"sequential_29\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_23 (RandomFlip)          │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_19 (RandomRotation)  │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_63 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_42 (MaxPooling2D)      │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_64 (Conv2D)                   │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_43 (MaxPooling2D)      │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_65 (Conv2D)                   │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_21 (Flatten)                 │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_42 (Dense)                     │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_43 (Dense)                     │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (Dropout)                  │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 95s 526ms/step - accuracy: 0.4902 - loss: 21.2286 - val_accuracy: 0.6144 - val_loss: 0.6761\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 212ms/step - accuracy: 0.5317 - loss: 1.4244 - val_accuracy: 0.6432 - val_loss: 0.6785\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 32s 218ms/step - accuracy: 0.5418 - loss: 1.4079 - val_accuracy: 0.6612 - val_loss: 0.6301\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 292ms/step - accuracy: 0.5442 - loss: 1.4034 - val_accuracy: 0.6797 - val_loss: 0.6092\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5717 - loss: 1.3642 - val_accuracy: 0.6922 - val_loss: 0.6192\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5531 - loss: 1.3825 - val_accuracy: 0.6823 - val_loss: 0.6084\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 30s 206ms/step - accuracy: 0.5540 - loss: 1.3716 - val_accuracy: 0.6660 - val_loss: 0.6346\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 212ms/step - accuracy: 0.5635 - loss: 1.3650 - val_accuracy: 0.6930 - val_loss: 0.6010\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 30s 206ms/step - accuracy: 0.5497 - loss: 1.3600 - val_accuracy: 0.7072 - val_loss: 0.5903\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5622 - loss: 1.3492 - val_accuracy: 0.6763 - val_loss: 0.6072\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.5645 - loss: 1.3489 - val_accuracy: 0.6870 - val_loss: 0.5993\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 210ms/step - accuracy: 0.5565 - loss: 1.3696 - val_accuracy: 0.6999 - val_loss: 0.6130\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5767 - loss: 1.3488 - val_accuracy: 0.6892 - val_loss: 0.5941\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 30s 209ms/step - accuracy: 0.5696 - loss: 1.3566 - val_accuracy: 0.6926 - val_loss: 0.6353\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 210ms/step - accuracy: 0.5674 - loss: 1.3447 - val_accuracy: 0.7283 - val_loss: 0.5621\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5713 - loss: 1.3509 - val_accuracy: 0.7356 - val_loss: 0.5911\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5638 - loss: 1.3601 - val_accuracy: 0.7210 - val_loss: 0.5733\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5623 - loss: 1.3656 - val_accuracy: 0.7270 - val_loss: 0.5433\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5904 - loss: 1.3121 - val_accuracy: 0.7395 - val_loss: 0.5454\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 212ms/step - accuracy: 0.5647 - loss: 1.3410 - val_accuracy: 0.7399 - val_loss: 0.5652\n\n\nWow! The accuracy of my model stabilized between 61.44% and 73.99% during training, meaning it was 6.07-18.62% better at predicting the label than our model1 was. And, if we take a look at the chart comparing the training and validation accuracy,\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nwe can see that the training accuracy was never higher than the validation accuracy, meaning there were no immediate observations of overfitting happening here, unlike model1.\n\n\nData Processing\nThe third model that we will create will continue building off of the previous models. Now, we will introduce the concept of data processing\nSometimes, itis helpful to make simple transformations to the input data to make training easier for neural network models. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0/-1 and 1 where we scale the weights. However, rather than scaling during training, we can do it prior which will allow the training time/energy more focused on the data content itself.\nThus, we can do that by writing the following block of code:\n\n# Define the preprocessing layer\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs=i, outputs=x)\n\nWith this preprocessor layer, we can slot it into our model pipeline, getting the following code:\n\n# Define the rest of the model architecture\nmodel3 = models.Sequential([\n    preprocessor,\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(2),\n])\n\nNow, let’s get our summary and train the model.\n\nmodel3.summary()\n\nModel: \"sequential_4\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ functional_13 (Functional)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_flip_4 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_4 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_12 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_8 (MaxPooling2D)       │ (None, 49, 49, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_13 (Conv2D)                   │ (None, 47, 47, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_9 (MaxPooling2D)       │ (None, 15, 15, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_14 (Conv2D)                   │ (None, 13, 13, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_4 (Flatten)                  │ (None, 10816)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_8 (Dense)                      │ (None, 64)                  │         692,288 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_9 (Dense)                      │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,163,176 (8.25 MB)\n\n\n\n Trainable params: 721,058 (2.75 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 1,442,118 (5.50 MB)\n\n\n\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.7971 - loss: 0.4250 - val_accuracy: 0.7932 - val_loss: 0.4449\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8115 - loss: 0.4147 - val_accuracy: 0.8001 - val_loss: 0.4289\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8085 - loss: 0.4122 - val_accuracy: 0.8027 - val_loss: 0.4295\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8054 - loss: 0.4186 - val_accuracy: 0.8035 - val_loss: 0.4274\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8019 - loss: 0.4157 - val_accuracy: 0.7975 - val_loss: 0.4353\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8083 - loss: 0.4125 - val_accuracy: 0.8057 - val_loss: 0.4303\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.8173 - loss: 0.4012 - val_accuracy: 0.8108 - val_loss: 0.4125\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8148 - loss: 0.3978 - val_accuracy: 0.8044 - val_loss: 0.4160\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8191 - loss: 0.3924 - val_accuracy: 0.7966 - val_loss: 0.4294\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8214 - loss: 0.3920 - val_accuracy: 0.8113 - val_loss: 0.4249\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8259 - loss: 0.3842 - val_accuracy: 0.8078 - val_loss: 0.4094\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8268 - loss: 0.3803 - val_accuracy: 0.8151 - val_loss: 0.4329\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8278 - loss: 0.3854 - val_accuracy: 0.8199 - val_loss: 0.4099\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8346 - loss: 0.3733 - val_accuracy: 0.8108 - val_loss: 0.4125\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8439 - loss: 0.3667 - val_accuracy: 0.8203 - val_loss: 0.3971\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8353 - loss: 0.3675 - val_accuracy: 0.8246 - val_loss: 0.4054\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8362 - loss: 0.3706 - val_accuracy: 0.8250 - val_loss: 0.3923\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.8477 - loss: 0.3479 - val_accuracy: 0.8177 - val_loss: 0.4211\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8384 - loss: 0.3572 - val_accuracy: 0.8250 - val_loss: 0.3840\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.8438 - loss: 0.3590 - val_accuracy: 0.8186 - val_loss: 0.3964\n\n\nThis one performed even better! The accuracy of my model stabilized between 79.32% an 82.50% during training, meaning it was better than model1 by 23.95-27.13%.\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nHowever, like model2 our training accuracy values are consistently higher than the validation accuracy values, indicating signs of overfitting for model3.\n\n\nTransfer Learning\nFor our last model, we will be focusing on tranfer learning. What does that mean?\nSo far, we’ve been training models for distinguishing between cats and dogs from scratch. In some cases, however, someone might already have trained a model that does a related task, and might have learned some relevant patterns. For example, folks train machine learning models for a variety of image recognition tasks. Thus, we can try to use a pre-existing model for our task.\nTo do this, we need to first access a pre-existing “base model” which we will then incorporate in a full model to train for our specific dataset. Thus, we will first use the following code to download MobileNetV2Large and then configure as a layer to be slotted in our model, similar to preprocessor:\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\n/usr/local/lib/python3.10/dist-packages/keras/src/applications/mobilenet_v3.py:512: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 ━━━━━━━━━━━━━━━━━━━━ 2s 0us/step\n\n\nFrom this “base model,” all we have to do is add in our audmentation layer as well as a Dense() layer for prediction.\n\nmodel4 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n    base_model_layer,\n    layers.GlobalMaxPool2D(),\n    layers.Dense(2)\n])\n\nFor one last time, let’s take a look at our summary and then start training.\n\nmodel4.summary()\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_1 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_1 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_1 (Functional)            │ (None, 5, 5, 960)           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_max_pooling2d_1               │ (None, 960)                 │               0 │\n│ (GlobalMaxPooling2D)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 2)                   │           1,922 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,998,274 (11.44 MB)\n\n\n\n Trainable params: 1,922 (7.51 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\nTwo things to note from this summary. The first is that between the base_model_layer, denoted by functional_1, and Dense() layer, there is a major jump in output shape, which is why we added a GlobalMaxPooling2D layer to help the transformation from 2D to 1D (needed for dense layer). The second thing is that compared to the previous models which had hundreds of thousands and even millions of training parameters, this model only has close to 2,000 traininable parameters, making this more efficient.\nWith that, let’s start the training.\n\nmodel4.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 25s 93ms/step - accuracy: 0.7369 - loss: 2.4357 - val_accuracy: 0.9467 - val_loss: 0.3232\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.8970 - loss: 0.6494 - val_accuracy: 0.9523 - val_loss: 0.3336\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9010 - loss: 0.6160 - val_accuracy: 0.9579 - val_loss: 0.2451\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9198 - loss: 0.4460 - val_accuracy: 0.9450 - val_loss: 0.3071\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.9076 - loss: 0.4425 - val_accuracy: 0.9622 - val_loss: 0.1932\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.9233 - loss: 0.3645 - val_accuracy: 0.9471 - val_loss: 0.2529\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.9129 - loss: 0.4052 - val_accuracy: 0.9553 - val_loss: 0.2149\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.9191 - loss: 0.3396 - val_accuracy: 0.9549 - val_loss: 0.2056\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.9128 - loss: 0.3522 - val_accuracy: 0.9665 - val_loss: 0.1436\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.9224 - loss: 0.3051 - val_accuracy: 0.9415 - val_loss: 0.2613\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9180 - loss: 0.3293 - val_accuracy: 0.9587 - val_loss: 0.1748\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9279 - loss: 0.2875 - val_accuracy: 0.9540 - val_loss: 0.1666\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9212 - loss: 0.3009 - val_accuracy: 0.9647 - val_loss: 0.1292\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.9303 - loss: 0.2481 - val_accuracy: 0.9527 - val_loss: 0.1534\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9217 - loss: 0.2870 - val_accuracy: 0.9604 - val_loss: 0.1454\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.9179 - loss: 0.2974 - val_accuracy: 0.9574 - val_loss: 0.1692\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9206 - loss: 0.2916 - val_accuracy: 0.9316 - val_loss: 0.3114\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9236 - loss: 0.2707 - val_accuracy: 0.9536 - val_loss: 0.1814\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 62ms/step - accuracy: 0.9258 - loss: 0.2644 - val_accuracy: 0.9574 - val_loss: 0.1585\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 53ms/step - accuracy: 0.9227 - loss: 0.2860 - val_accuracy: 0.9536 - val_loss: 0.1625\n\n\nThis one did the best, constantly hitting over 90’s! The accuracy of my model stabilized between 93.16% and 96.65% during training, meaning our model is 37.79-41.28% better at predicting than model1. Comparing the accuracy values,\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nvalidation again is always higher than training, indicating no immediae signs of overfitting for model4.\n\n\nTrying on Test Data\nAgain, out of all of these, it seems that model4 was the most performant model, given it has only 90’s values for validation accuracy and there are no indicators of overfitting.\nNow that we have our best model figured out, we will now run our model against unseen test data.\n\nhistory=model4.fit(test_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 142ms/step - accuracy: 0.9189 - loss: 0.3080 - val_accuracy: 0.9536 - val_loss: 0.1858\nEpoch 2/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 83ms/step - accuracy: 0.9101 - loss: 0.3349 - val_accuracy: 0.9549 - val_loss: 0.1662\nEpoch 3/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 82ms/step - accuracy: 0.9169 - loss: 0.2920 - val_accuracy: 0.9673 - val_loss: 0.1224\nEpoch 4/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 84ms/step - accuracy: 0.9159 - loss: 0.3279 - val_accuracy: 0.9647 - val_loss: 0.1407\nEpoch 5/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 84ms/step - accuracy: 0.9232 - loss: 0.2747 - val_accuracy: 0.9660 - val_loss: 0.1449\nEpoch 6/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 82ms/step - accuracy: 0.9433 - loss: 0.2147 - val_accuracy: 0.9622 - val_loss: 0.1385\nEpoch 7/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 87ms/step - accuracy: 0.9496 - loss: 0.1960 - val_accuracy: 0.9639 - val_loss: 0.1319\nEpoch 8/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 84ms/step - accuracy: 0.9408 - loss: 0.1987 - val_accuracy: 0.9609 - val_loss: 0.1518\nEpoch 9/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 85ms/step - accuracy: 0.9387 - loss: 0.2129 - val_accuracy: 0.9527 - val_loss: 0.1887\nEpoch 10/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 84ms/step - accuracy: 0.9329 - loss: 0.2163 - val_accuracy: 0.9635 - val_loss: 0.1364\nEpoch 11/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 82ms/step - accuracy: 0.9322 - loss: 0.2130 - val_accuracy: 0.9592 - val_loss: 0.1527\nEpoch 12/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 89ms/step - accuracy: 0.9350 - loss: 0.1861 - val_accuracy: 0.9475 - val_loss: 0.2041\nEpoch 13/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 85ms/step - accuracy: 0.9348 - loss: 0.2025 - val_accuracy: 0.9570 - val_loss: 0.1802\nEpoch 14/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 87ms/step - accuracy: 0.9428 - loss: 0.1874 - val_accuracy: 0.9566 - val_loss: 0.1601\nEpoch 15/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 84ms/step - accuracy: 0.9472 - loss: 0.1735 - val_accuracy: 0.9613 - val_loss: 0.1464\nEpoch 16/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 85ms/step - accuracy: 0.9377 - loss: 0.1891 - val_accuracy: 0.9613 - val_loss: 0.1472\nEpoch 17/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 85ms/step - accuracy: 0.9403 - loss: 0.1884 - val_accuracy: 0.9652 - val_loss: 0.1342\nEpoch 18/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 85ms/step - accuracy: 0.9561 - loss: 0.1471 - val_accuracy: 0.9660 - val_loss: 0.1273\nEpoch 19/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 5s 87ms/step - accuracy: 0.9295 - loss: 0.2089 - val_accuracy: 0.9635 - val_loss: 0.1324\nEpoch 20/20\n37/37 ━━━━━━━━━━━━━━━━━━━━ 3s 84ms/step - accuracy: 0.9287 - loss: 0.2311 - val_accuracy: 0.9540 - val_loss: 0.1750\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"testing\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nRunning the test and validation dataset together, we can see that “the accuracy of the model stabilized between 94.75% and 96.73%”, and the testing accuracy values are lower than the validation accuracy values, similar results when training model4."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pic16bblog",
    "section": "",
    "text": "Homework 5: TensorFlow and Keras Modeling (Image Classification)\n\n\n\n\n\n\nweek 8\n\n\nHomework\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Matrix Multiplication and Jax Numpy (Heat Diffusion)\n\n\n\n\n\n\nweek 7\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/bruin/HW 4/index.html",
    "href": "posts/bruin/HW 4/index.html",
    "title": "Homework 4: Matrix Multiplication and Jax Numpy (Heat Diffusion)",
    "section": "",
    "text": "In this blog post, we will be going over how to use numpy arrays as well as comparing special tools to work with linear algebra. For our example, we will be conducting a simulation of two-dimensional heat diffusion maps.\nBecause we will be working with numpy as well as data visualization, we will need to import both numpy as well as pyplot.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nFor our 2D-heat diffusion maps, we will be using the following parameters:\n\nN = 101\nepsilon = 0.2\niters = 2700\n\nHere, our N represents the size of our array (meaning there will be N^2 values in our array) epsilon represents the “heaviness / stability” of each of our update, and iters represents the amount of iterations our simulation will run.\nFirst, we will need to create an an initial heatmap to iterate and update upon. We can create an initial N x N empty grid with a singular unit of heat at the center with the following line of code:\n\nu0 = np.zeros((N, N))              # Creating NxN numpy array with entries of 0\nu0[int(N/2), int(N/2)] = 1.0       # Assigning the midpoint value of initial array to 1\nplt.imshow(u0)                     # Plotting numpy array with the 1 unit of heat at midpoint\n\n\n\n\n\n\n\n\nGreat! Now that we have our initical condition heatmap, we can using different methods to update it.\n\nMatrix Multiplication\nThe first method we will use is matrix multiplication. Essentially, we want to construct an updating matrix A that will act upon the flattened version of u which will be iterated and updated each time.\nThus, the first function we want to create is for constructing our update matrix A (this matrix is called a finite difference matrix). We want our update in discrete time to be represented by the following function:\n\n\n\nScreenshot 2024-02-23 1.31.17 PM.png\n\n\nThus, we want our updating matrix A to represent the paranthesis part that is multiplied to epsilon.\n\n\n\nScreenshot 2024-02-23 1.30.29 PM.png\n\n\nHere, each u will represent a diagonal in our matrix: * u[i+1][j]: represents nth upper diagonal * u[i-1][j]: represents nth lower diagonal * u[i][j+1]: represents 1st upper diagonal * u[i][j-1]: represents 1st lower diagonal * u[i][j]: main diagonal\nWith this information, we can now create our function to account for the desired grid size N, which should look something like this:\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nimport sys\nsys.path.append('/content/gdrive/My Drive')\nimport inspect\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\nDrive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\ndef get_A(N):\n  \"\"\"Creates a 2D finite difference matrix by adjusting the diagonal entries\n  Args:\n       N: Integer representing size of desired matrix\n\n  Returns:\n       N x N adjusting matrix.\n  \"\"\"\n  # Getting total number of grid points\n  n = N * N\n  # Creating list with representative diagonals\n  diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n\n  # Adjusting diagonals to account periodic 0's\n  diagonals[1][(N-1)::N] = 0\n  diagonals[2][(N-1)::N] = 0\n\n  # Constructing finite difference matrix\n  A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n  return A\n\n\n\nNow that we have our function, let’s see what our function looks like with a test value parameter. For simplicity, we will use 3.\n\nget_A(3)\n\narray([[-4.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 1., -4.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  1., -4.,  0.,  0.,  1.,  0.,  0.,  0.],\n       [ 1.,  0.,  0., -4.,  1.,  0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  1., -4.,  1.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.,  1., -4.,  0.,  0.,  1.],\n       [ 0.,  0.,  0.,  1.,  0.,  0., -4.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  1.,  0.,  1., -4.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  1., -4.]])\n\n\nOur code outputted the matrix that we were looking for. Now, we can start on our next step: updating u.\nTo match the discrete time update equation, we want to take into account 3 parameters: our update matrix A, the iterated matrix u, and our “scaling/stability” value epsilon. Because we already have all the components of the equation. We can just type out the equation in our function. So our function should look something like this:\n\nfrom heat_equation import advance_time_matvecmul\nimport inspect\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]                                        # Calculating size of grid\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))   # Flattening 2D grid 'u', Matrix mult. with\n                                                          # A, Reshaping back to original grade shape,\n                                                          # Updating 'u' with matrix multiplication + epsilon\n    return u\n\n\n\nWith both of these equations, we can now run our iterations.\nFirst, because we are comparing different methods for creating identical heatmaps, we want to import time in order to see execution time.\n\nimport time\n\nNext, we want to create each of our variable in order to be able to use the advance_time_matvecmul function. We already have epsilon established earlier, so we just need A and u. Because we want to see the development of the grid with each update, we want to create a list to append the u at different iterations. Thus, we will use the following block of code:\n\nA = get_A(N)\nu = u0\nu_array = []\n\n# Starting execution time for iterations\nstart_time = time.time()\n\nfor i in range(1,iters+1):\n  u = advance_time_matvecmul(A, u, epsilon)             # Updating our heatmap grid\n  if i % 300 == 0:\n    u_array.append(u.copy())                            # Appending every 300-th heatmap grid\n\n# Calculating iteration time\nexecution_time = time.time() - start_time\nprint(\"Execution Time is \", execution_time, \" seconds\")\n\nExecution Time is  233.45097470283508  seconds\n\n\nThis approach took quite a long time, taking almost 4 minutes to go through 2700 iterations.\nAlso note that through the conditional statement, we only appended 9 versions of u. With this appended list, we can now visualize the diffusion heatmaps. Because we have 9 grids, we can do 3x3 subplots by using pyplot through the following block of code:\n\ndef diffusion_visualization(list):\n  \"\"\"Generating a visualization of the 9 developed heatmap grids\n  Args:\n      list: list of appended heatmap grids\n\n  Returns:\n      3 rows, 3 columns of subplots of the heatmap grids\n  \"\"\"\n  # Generating base subplots\n  fig, ax = plt.subplots(3, 3, figsize = (12, 12))\n\n  # Creating indexing counter for list\n  count = 0\n\n  # Looping through row\n  for i in range(3):\n    # Looping through column\n    for j in range(3):\n      # Plotting/Showing the nth item in the list\n      ax[i][j].imshow(list[count])\n      # Setting title of the nth item in the list\n      ax[i][j].set_title(f\"{(count+1)*300}th iteration\")\n      # Iterator\n      count += 1\n\n  plt.tight_layout()\n  plt.show()\n\nGreat! Now that we have our diffusion heat visualization function, let’s run it using our list of wanted iterations.\n\ndiffusion_visualization(u_array)\n\n\n\n\n\n\n\n\nNotice that as more iterations occur, the heatmap develops, becoming more vibrant with the circles getting wider (exactly what we want).\n\n\nSparse Matrix in JAX\nNow that we have our initial method down, let’s use another method to increase computational time. For this one, we will use sparse matrices from JAX, a Python library designed for high-performance numerical computing.\nSo first, we need to import the following:\n\nimport jax\nfrom jax.experimental import sparse           # Allowing us to create sparse matrices\nimport jax.numpy as jnp                       # Allowing us to utilize jax numpy\nfrom jax import jit                           # Allowing us to use Just-In-Time compilations\n                                              # to execute JAX function effectively\n\n\nThe next step we want to do is create a function to output us a sparse matrix. Because we already have the function get_A to help us output a finite difference matrix, we can use call get_A in this function to create the array A, which we will then turn into a JAX numpy sparse array. We can do that with the following block of code:\n\nfrom heat_equation import get_sparse_A\nimport inspect\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n  \"\"\" Advancing simulation through BCOO (JAX batched coordimate)\n  to create sparse updating matrix\n\n  Args:\n    N: size of array\n  Returns:\n    an N x N sparse updating matrix\n  \"\"\"\n\n  # Creating updating matrix\n  A = get_A(N)\n  # Transforming A into JAX numpy array\n  jnp_A = jnp.array(A)\n  # Converting into sparse matrix\n  A_sp_matrix = sparse.BCOO.fromdense(jnp_A)\n  return A_sp_matrix\n\n\n\nNow that we have our new sparse matrix, we can repeat the process of iteration and visualization. However, we also want to use the jit-ed version of advance_time_matvecmul to execute the JAX numpys for effectively. Thus our code should look something like this:\n\n# Getting elements for advance_time function and iteration for loop\nA_sp_matrix = get_sparse_A(N)\nu = u0\nu_array_sparse = []\n\n# Using JIT compilation for advance_time function\njitted_advance_time_matvecmul = jax.jit(advance_time_matvecmul)\n\nstart_time = time.time()\n\nfor i in range(1,iters+1):\n  u = jitted_advance_time_matvecmul(A_sp_matrix, u, epsilon)\n  if i % 300 ==0:\n    u_array_sparse.append(u.copy())\n\nexecution_time = time.time() - start_time\nprint(\"Execution Time is \", execution_time, \" seconds\")\n\ndiffusion_visualization(u_array_sparse)\n\nExecution Time is  1.3260242938995361  seconds\n\n\n\n\n\n\n\n\n\nNotice that this function is MUCH faster than the first process! It took only 1.33 seconds.\n\n\nDirect Operation with Numpy\nFor this method, rather than relying on an updating matrix, we can update matrix u itself by using numpy’s vectorized array operations.\nTherefore, we will create an updated version of advance_time_vatmecmul that only needs two parameters: u and epsilon.\nAgain, we want our function to execute the updating equation above. Except this time. Rather than writing a matrix to represent the paranthetical part. we can use numpy to represent each u[i+1][j], u[i-1][j], u[i][j+1], u[i][j-1].\nWe can do this with using np.roll(), where we shift each of the points either left/right or up/down, depending on which we need. However, an important factor of np.roll() that we need to know is that it also takes the last point in the array and makes it the first and vice versa, which we don’t want. Thus we need to pad our array with 0 along the edge to prevent this. Thus our code should look something like this:\n\nfrom heat_equation import advance_time_numpy\nimport inspect\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n  \"\"\"Advances the simulation by one timestep, via numpy rolling\n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n  \"\"\"\n\n  # Creating padded version of u with 0's along edge\n  padded_u = np.pad(u, 1, mode = 'constant')\n\n  # Creating rolled version of padded u\n  right_roll = np.roll(padded_u, 1, axis = 1)\n  left_roll = np.roll(padded_u, -1, axis = 1)\n  up_roll = np.roll(padded_u, -1, axis = 0)\n  down_roll = np.roll(padded_u, 1, axis = 0)\n\n  # Updating u\n  numpy_u = padded_u + epsilon*(right_roll + left_roll + up_roll + down_roll - (4*padded_u))\n  # Removing the 0's on the edge\n  numpy_u_sliced = numpy_u[1:-1, 1:-1]\n  return numpy_u_sliced\n\n\n\nNotice that before returning the updated u, we used sliced indexing because the padded u is a (N+2) x (N+2) array, but we only want a N x N array.\nWith this function, we can again run our iterations and visualization and see how long this takes.\n\nnumpy_u = u0\nresults_numpy = []\n\nstart_time = time.time()\n\nfor i in range(1,iters+1):\n  numpy_u = advance_time_numpy(numpy_u, epsilon)\n  if i % 300 ==0:\n    results_numpy.append(numpy_u.copy())\n\nexecution_time = time.time() - start_time\nprint(\"Execution Time is \", execution_time, \" seconds\")\n\ndiffusion_visualization(results_numpy)\n\nExecution Time is  0.522552490234375  seconds\n\n\n\n\n\n\n\n\n\nThis time, it only takes 0.52 seconds, 2.3 times as fast!\n\n\nJAX\nThe last method we will use is jax, similar to when we did sparse matrix, we will do an jax version of the previous function, which will allow us to use just-in-time compilation. Benefit of this method as well is that we will not need to rely on (sparse) matrix multiplication.\nEverything will be the same as advance_time_numpy, but we just replace all the numpy with jax.numpy, or jnp. Thus we should get the following code:\n\nfrom heat_equation import advance_time_jax\nimport inspect\nprint(inspect.getsource(advance_time_jax))\n\ndef advance_time_jax(u, epsilon):\n  \"\"\"Advances the simulation by one timestep, via jax numpy rolling\n  Args:\n      u: N x N grid state at timestep k.\n      epsilon: stability constant.\n\n  Returns:\n      N x N Grid state at timestep k+1.\n  \"\"\"\n\n  # Creating padded version of u with 0's along edge\n  padded_u = jnp.pad(u, 1, mode = 'constant')\n\n  # Creating rolled version of padded u\n  right_roll = jnp.roll(padded_u, 1, axis = 1)\n  left_roll = jnp.roll(padded_u, -1, axis = 1)\n  up_roll = jnp.roll(padded_u, -1, axis = 0)\n  down_roll = jnp.roll(padded_u, 1, axis = 0)\n\n  # Updating u\n  jnp_u = padded_u + epsilon*(right_roll + left_roll + up_roll + down_roll - (4*padded_u))\n  # Removing the 0's on the edge\n  jnp_u_sliced = jnp_u[1:-1, 1:-1]\n  return jnp_u_sliced\n\n\n\nFor one last time, we will run our iteration and visualization. Because we are dealing with jax numpy again, we will use the jit-ed version of advance_time_jax.\n\njitted_jax = jax.jit(advance_time_jax)\njnp_u = u0\nresults_jnp = []\n\nstart_time = time.time()\n\nfor i in range(1,iters+1):\n  jnp_u = jitted_jax(jnp_u, epsilon)\n  if i % 300 ==0:\n    results_jnp.append(jnp_u.copy())\n\nexecution_time = time.time() - start_time\nprint(\"Execution Time is \", execution_time, \" seconds\")\n\ndiffusion_visualization(results_jnp)\n\nExecution Time is  0.08782696723937988  seconds\n\n\n\n\n\n\n\n\n\nThis one is the fastest of them all, taking 0.09 seconds, more than 5 times as fast as the previous one!\n\n\nComparison\nOut of all of these, the fastest is of course with jax as it took only 0.09 seconds, followed by direct operation through numpy, then sparse matrix, and lastly direct matrix multiplication.\nOut of all of these, I think that direct operation through nunmpy was the easiest to write since it did not require me to write two functions, and I did not have to use an additional updating matrix, which got really complicated."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]